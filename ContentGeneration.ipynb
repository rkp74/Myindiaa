{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0zJblyNe8nEpGH/eVdKIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkp74/Myindiaa/blob/main/ContentGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Template Based Generation"
      ],
      "metadata": {
        "id": "4r0E75l1RobM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI API"
      ],
      "metadata": {
        "id": "oZsRbbuUhdtG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-QnJJzGB6_3",
        "outputId": "646c7d0b-0289-4e72-e75a-db78744a7e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community --quiet\n",
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "ny8VC1IeGGDa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "# Prompt the user to input their API key\n",
        "api_key = getpass.getpass('Please enter your OpenAI API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRa1EXpqFYRv",
        "outputId": "1cf6ef28-be38-4fb6-da63-9d37b2068d8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "QjwbDid6-cU5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"attributes\"],\n",
        "    template=\"Generate a product description based on the following attributes:\\n{attributes}\"\n",
        ")"
      ],
      "metadata": {
        "id": "7MkhCDUKIPn0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate product descriptions\n",
        "def generate_product_description(attributes, model_name=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Generate a product description using OpenAI's GPT model based on provided product attributes.\n",
        "\n",
        "    :param attributes: Dictionary containing product attributes\n",
        "    :param model_name: Name of the OpenAI model to use\n",
        "    :return: Generated product description as a string\n",
        "    \"\"\"\n",
        "    # Convert attributes dictionary to a formatted string\n",
        "    attributes_str = \"\\n\".join(f\"{key}: {value}\" for key, value in attributes.items())\n",
        "\n",
        "    # Initialize the OpenAI model for chat\n",
        "    llm = ChatOpenAI(model=model_name)\n",
        "\n",
        "    # Create a RunnableSequence\n",
        "    sequence = RunnableSequence(prompt_template | llm)\n",
        "\n",
        "    # Generate the product description\n",
        "    description = sequence.invoke({\"attributes\": attributes_str})\n",
        "\n",
        "    return description"
      ],
      "metadata": {
        "id": "Rc_3JaEXITh_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example product attributes\n",
        "product_attributes = {\n",
        "    \"Product Name\": \"Eco-Friendly Water Bottle\",\n",
        "    \"Material\": \"Stainless Steel\",\n",
        "    \"Capacity\": \"750ml\",\n",
        "    \"Color\": \"Ocean Blue\",\n",
        "    \"Features\": \"BPA-free, Leak-proof, Insulated\",\n",
        "    \"Target Audience\": \"Outdoor Enthusiasts, Eco-conscious Consumers\",\n",
        "    \"Brand\": \"GreenLife\"\n",
        "}"
      ],
      "metadata": {
        "id": "kcJl7N5VFwHZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the product description using a specific model\n",
        "description = generate_product_description(product_attributes, model_name=\"gpt-3.5-turbo\")\n",
        "print(\"Generated Product Description (gpt-3.5-turbo):\\n\")\n",
        "print(description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuIzZJUuIh3b",
        "outputId": "379c1ca4-3967-43a1-a011-d62d1d2a97d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Product Description (gpt-3.5-turbo):\n",
            "\n",
            "content='Introducing the GreenLife Eco-Friendly Water Bottle, the perfect companion for outdoor enthusiasts and eco-conscious consumers. Made from high-quality stainless steel in a stunning ocean blue color, this 750ml bottle is both stylish and sustainable.\\n\\nStay hydrated on-the-go with confidence, as our water bottle is BPA-free and leak-proof, ensuring a mess-free experience every time. The insulated design keeps your drinks at the perfect temperature for hours, making it ideal for hiking, camping, or simply enjoying a day outdoors.\\n\\nChoose the GreenLife Eco-Friendly Water Bottle and make a positive impact on the environment while staying hydrated in style.' response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 71, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-47d14711-8133-465d-9207-3d53fd921ced-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Transformers"
      ],
      "metadata": {
        "id": "efX0e76xhh04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-huggingface transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xWLC7FibO2s",
        "outputId": "ebb2546d-24bd-428f-d984-66ec66b28db5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "HmCAPuPFH4uJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "# Prompt the user to input their API key\n",
        "hf_api_key = getpass.getpass('Please enter your Hugging Face API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkHbjE4fH4Hj",
        "outputId": "1fc4ca24-310c-46cd-93b3-11d0130036b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Hugging Face API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_key\n"
      ],
      "metadata": {
        "id": "CeKxC2RFH0cH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate product descriptions using Hugging Face Transformers\n",
        "def generate_product_description_hf(attributes, model_name=\"EleutherAI/gpt-neo-2.7B\"):\n",
        "    \"\"\"\n",
        "    Generate a product description using Hugging Face Transformers based on provided product attributes.\n",
        "\n",
        "    :param attributes: Dictionary containing product attributes\n",
        "    :param model_name: Hugging Face model to use\n",
        "    :return: Generated product description as a string\n",
        "    \"\"\"\n",
        "    # Convert attributes dictionary to a formatted string\n",
        "    attributes_str = \"\\n\".join(f\"{key}: {value}\" for key, value in attributes.items())\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"Generate a detailed and engaging product description for the following product based on its attributes:\\n\\n{attributes_str}\\n\\nProduct Description:\"\n",
        "\n",
        "    # Load the Hugging Face model and tokenizer\n",
        "    generator = pipeline('text-generation', model=model_name, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    # Generate the product description\n",
        "    response = generator(prompt, max_length=300, num_return_sequences=1, do_sample=True, truncation=True)\n",
        "\n",
        "    # Extract the generated text\n",
        "    description = response[0]['generated_text']\n",
        "\n",
        "    # Remove the prompt from the generated text\n",
        "    description = description.replace(prompt, \"\").strip()\n",
        "\n",
        "    return description"
      ],
      "metadata": {
        "id": "OQNwzelQa6mL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example product attributes\n",
        "product_attributes = {\n",
        "    \"Product Name\": \"Eco-Friendly Water Bottle\",\n",
        "    \"Material\": \"Stainless Steel\",\n",
        "    \"Capacity\": \"750ml\",\n",
        "    \"Color\": \"Ocean Blue\",\n",
        "    \"Features\": \"BPA-free, Leak-proof, Insulated\",\n",
        "    \"Target Audience\": \"Outdoor Enthusiasts, Eco-conscious Consumers\",\n",
        "    \"Brand\": \"GreenLife\"\n",
        "}"
      ],
      "metadata": {
        "id": "ITevEiueQCsp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the product description using Hugging Face Transformers\n",
        "description_hf = generate_product_description_hf(product_attributes, model_name=\"EleutherAI/gpt-neo-2.7B\")\n",
        "print(\"Generated Product Description:\\n\", description_hf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXQvWNBkbA0-",
        "outputId": "66bf9ff1-ecc6-4a80-da6f-c495d1ec7201"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Product Description:\n",
            " A water bottle is designed to carry as much liquid as possible by its height, so that the user can drink or take a drink with a smooth taste and feel, regardless of whether the user is running, cycling, hiking, mountain climbing or the like. The size and shape of a water bottle can be different, and the material of the bottle can be stainless steel, aluminum, plastic, glass, etc. A water bottle is widely used in sports and outdoor, and many water bottles are water bottles for sports. According to the size and shape of the bottle, the product can be divided into a large water bottle for large use and a water bottle for small use. At the same time, the function of water bottles is to carry out drink, so the type of water bottles for use varies from small water bottles for drinking to large water bottles for taking a drink. By being divided into two types, the product has good value added functions of being light, water-tight, long-lasting and low-maintenance. By being light, the product is easy to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule Based Generation"
      ],
      "metadata": {
        "id": "HjPS5BkBRurp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "RE5zV7AzYdjK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule-based content generation\n",
        "def generate_content_rule_based(attributes):\n",
        "    product_name = attributes.get(\"Product Name\", \"\")\n",
        "    material = attributes.get(\"Material\", \"\")\n",
        "    capacity = attributes.get(\"Capacity\", \"\")\n",
        "    color = attributes.get(\"Color\", \"\")\n",
        "    features = attributes.get(\"Features\", \"\")\n",
        "    audience = attributes.get(\"Target Audience\", \"\")\n",
        "    brand = attributes.get(\"Brand\", \"\")\n",
        "\n",
        "    template = f\"Introducing the {brand} {product_name}, a {capacity} {material} bottle in {color}. Perfect for {audience}. Features include {features}.\"\n",
        "\n",
        "    return template"
      ],
      "metadata": {
        "id": "j7sONpxXRxxI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example product attributes\n",
        "product_attributes = {\n",
        "    \"Product Name\": \"Eco-Friendly Water Bottle\",\n",
        "    \"Material\": \"Stainless Steel\",\n",
        "    \"Capacity\": \"750ml\",\n",
        "    \"Color\": \"Ocean Blue\",\n",
        "    \"Features\": \"BPA-free, Leak-proof, Insulated\",\n",
        "    \"Target Audience\": \"Outdoor Enthusiasts, Eco-conscious Consumers\",\n",
        "    \"Brand\": \"GreenLife\"\n",
        "}"
      ],
      "metadata": {
        "id": "K6_5L8YDSHad"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rule_based_content = generate_content_rule_based(product_attributes)"
      ],
      "metadata": {
        "id": "aEr3T1MpSDh6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rule-Based Content Generation:\")\n",
        "print(rule_based_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5B3YMYUSEJh",
        "outputId": "a99e006b-1151-4353-d283-8dfea7c42f57"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rule-Based Content Generation:\n",
            "Introducing the GreenLife Eco-Friendly Water Bottle, a 750ml Stainless Steel bottle in Ocean Blue. Perfect for Outdoor Enthusiasts, Eco-conscious Consumers. Features include BPA-free, Leak-proof, Insulated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Based (LSTM)"
      ],
      "metadata": {
        "id": "BwC4DnT6SyzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "AkFTlk9uYi2m"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample product attributes (replace with your actual data)\n",
        "product_attributes = [\n",
        "    {\"color\": \"blue\", \"material\": \"stainless steel\", \"capacity\": \"750ml\"},\n",
        "    {\"color\": \"green\", \"material\": \"plastic\", \"capacity\": \"500ml\"},\n",
        "    {\"color\": \"red\", \"material\": \"glass\", \"capacity\": \"1 liter\"}\n",
        "]"
      ],
      "metadata": {
        "id": "gvCfbYhzYkry"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize attributes\n",
        "def tokenize_attributes(attributes):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(attributes)\n",
        "    total_words = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
        "    return tokenizer, total_words"
      ],
      "metadata": {
        "id": "1At-e_ZXYo9t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate sequences from attributes\n",
        "def generate_sequences(tokenizer, attributes):\n",
        "    sequences = tokenizer.texts_to_sequences([f\"{key}: {value}\" for attr in attributes for key, value in attr.items()])\n",
        "    return sequences"
      ],
      "metadata": {
        "id": "XnkJDK--YrWL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad input sequences\n",
        "def pad_input_sequences(sequences):\n",
        "    max_sequence_len = max([len(seq) for seq in sequences])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n",
        "    return padded_sequences, max_sequence_len"
      ],
      "metadata": {
        "id": "oCyKQOJmYti9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build LSTM model\n",
        "def build_lstm_model(total_words, max_sequence_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))  # Embedding layer\n",
        "    model.add(LSTM(150))  # LSTM layer with 150 units\n",
        "    model.add(Dense(total_words, activation='softmax'))  # Dense output layer with softmax activation\n",
        "    adam = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "tSi1fh4LYt3K"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train LSTM model\n",
        "def train_lstm_model(model, xs, ys, epochs=100):\n",
        "    model.fit(xs, ys, epochs=epochs, verbose=1)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "YXwtQ9nqYy46"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text based on trained model\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        token_list = seed_text.split()[1:]  # Remove \"Attributes:\" and take the rest as tokens\n",
        "        token_list = tokenizer.texts_to_sequences([\" \".join(token_list)])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict probabilities for each word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Get the index of the word with the highest probability\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Convert index to word\n",
        "        predicted_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                predicted_word = word\n",
        "                break\n",
        "\n",
        "        # Handle special case for non-integer tokens (product attribute values)\n",
        "        if predicted_word is None:\n",
        "            predicted_word = token_list[0][0]  # Use the first token as the predicted word\n",
        "\n",
        "        generated_text += \" \" + str(predicted_word)\n",
        "\n",
        "    # Convert the tokenized indices back to text\n",
        "    final_text = \"Attributes:\"\n",
        "    for token in generated_text.split()[1:]:\n",
        "        try:\n",
        "            if token.isdigit():\n",
        "                word = tokenizer.index_word[int(token)]\n",
        "            else:\n",
        "                word = token\n",
        "            final_text += f\" {word}\"\n",
        "        except (KeyError, ValueError):\n",
        "            final_text += f\" {token}\"\n",
        "\n",
        "    return final_text\n",
        "\n"
      ],
      "metadata": {
        "id": "_EmytAzKY05q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate product descriptions\n",
        "def generate_product_descriptions(attributes, tokenizer, model, max_sequence_len):\n",
        "    sequences = generate_sequences(tokenizer, attributes)\n",
        "    padded_sequences, _ = pad_input_sequences(sequences)\n",
        "\n",
        "    descriptions = []\n",
        "    for seq in padded_sequences:\n",
        "        seed_text = \"Attributes:\"\n",
        "        for token in seq:\n",
        "            seed_text += f\" {token}\"\n",
        "        generated_text = generate_text(seed_text, 20, model, max_sequence_len, tokenizer)\n",
        "        descriptions.append(generated_text)\n",
        "\n",
        "    return descriptions"
      ],
      "metadata": {
        "id": "AN-S5Dh-Y5H_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main script\n",
        "def main():\n",
        "    # Tokenize attributes\n",
        "    tokenizer, total_words = tokenize_attributes([f\"{key}: {value}\" for attr in product_attributes for key, value in attr.items()])\n",
        "\n",
        "    # Generate sequences and pad input\n",
        "    sequences = generate_sequences(tokenizer, product_attributes)\n",
        "    padded_sequences, max_sequence_len = pad_input_sequences(sequences)\n",
        "\n",
        "    # Create predictors and labels\n",
        "    xs, labels = padded_sequences[:,:-1], padded_sequences[:,-1]\n",
        "    ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "    # Build and train LSTM model\n",
        "    lstm_model = build_lstm_model(total_words, max_sequence_len)\n",
        "    lstm_model = train_lstm_model(lstm_model, xs, ys)\n",
        "\n",
        "    # Generate product descriptions\n",
        "    descriptions = generate_product_descriptions(product_attributes, tokenizer, lstm_model, max_sequence_len)\n",
        "\n",
        "    # Print generated descriptions\n",
        "    for i, desc in enumerate(descriptions):\n",
        "        print(f\"Generated Description {i+1}: {desc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuNpz79xSyWk",
        "outputId": "80f846ca-af99-4027-de06-f0c866f20288"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.7088 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.6823 - accuracy: 0.2222\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6471 - accuracy: 0.3333\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.5958 - accuracy: 0.3333\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.5247 - accuracy: 0.3333\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4335 - accuracy: 0.3333\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3277 - accuracy: 0.3333\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2205 - accuracy: 0.3333\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1257 - accuracy: 0.3333\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0428 - accuracy: 0.4444\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9600 - accuracy: 0.4444\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8707 - accuracy: 0.4444\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7719 - accuracy: 0.4444\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6591 - accuracy: 0.5556\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5379 - accuracy: 0.5556\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4245 - accuracy: 0.5556\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3117 - accuracy: 0.5556\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2005 - accuracy: 0.5556\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0906 - accuracy: 0.5556\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9864 - accuracy: 0.5556\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9025 - accuracy: 0.5556\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8367 - accuracy: 0.5556\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7867 - accuracy: 0.5556\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7496 - accuracy: 0.5556\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7232 - accuracy: 0.5556\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7044 - accuracy: 0.5556\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6930 - accuracy: 0.5556\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6861 - accuracy: 0.5556\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6813 - accuracy: 0.5556\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6795 - accuracy: 0.5556\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6771 - accuracy: 0.5556\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6770 - accuracy: 0.5556\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6757 - accuracy: 0.5556\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6759 - accuracy: 0.5556\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6752 - accuracy: 0.5556\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6753 - accuracy: 0.5556\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6751 - accuracy: 0.5556\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6750 - accuracy: 0.5556\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6747 - accuracy: 0.5556\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6750 - accuracy: 0.5556\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6748 - accuracy: 0.5556\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6747 - accuracy: 0.5556\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6747 - accuracy: 0.5556\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6743 - accuracy: 0.5556\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6748 - accuracy: 0.5556\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6750 - accuracy: 0.5556\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6749 - accuracy: 0.5556\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6743 - accuracy: 0.5556\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6743 - accuracy: 0.5556\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6744 - accuracy: 0.5556\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6745 - accuracy: 0.5556\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6746 - accuracy: 0.5556\n",
            "Generated Description 1: Attributes: 0 color blue 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml\n",
            "Generated Description 2: Attributes: material stainless steel blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n",
            "Generated Description 3: Attributes: 0 capacity 750ml blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n",
            "Generated Description 4: Attributes: 0 color green 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml\n",
            "Generated Description 5: Attributes: 0 material plastic blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n",
            "Generated Description 6: Attributes: 0 capacity 500ml blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n",
            "Generated Description 7: Attributes: 0 color red 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml 750ml\n",
            "Generated Description 8: Attributes: 0 material glass blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n",
            "Generated Description 9: Attributes: capacity 1 liter blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue blue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A/B Testing"
      ],
      "metadata": {
        "id": "8Qs54-h0oXCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothetical results\n",
        "results = {\n",
        "    \"Description 1\": {\"CTR\": 0.05, \"Conversion Rate\": 0.02, \"Engagement\": 50},\n",
        "    \"Description 2\": {\"CTR\": 0.07, \"Conversion Rate\": 0.03, \"Engagement\": 70},\n",
        "    \"Description 3\": {\"CTR\": 0.04, \"Conversion Rate\": 0.01, \"Engagement\": 40},\n",
        "    \"Description 4\": {\"CTR\": 0.06, \"Conversion Rate\": 0.025, \"Engagement\": 60}\n",
        "}\n",
        "\n",
        "# Determine the best performing description\n",
        "best_description = max(results, key=lambda k: (results[k][\"CTR\"], results[k][\"Conversion Rate\"], results[k][\"Engagement\"]))\n",
        "print(f\"The best performing description is: {best_description} with metrics: {results[best_description]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wUKWR_-oUtJ",
        "outputId": "383d3c59-c614-4ebf-c850-28e3935d701c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best performing description is: Description 2 with metrics: {'CTR': 0.07, 'Conversion Rate': 0.03, 'Engagement': 70}\n"
          ]
        }
      ]
    }
  ]
}